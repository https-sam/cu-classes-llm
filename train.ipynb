{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e7b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install sagemaker langchain amazon-textract-caller amazon-textract-textractor sentence-transformers pypdf pip install faiss-cpu -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91613d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/osgo1115exc/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3, json, sagemaker\n",
    "from typing import Dict\n",
    "from langchain import LLMChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from sagemaker.huggingface import HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ff459",
   "metadata": {},
   "source": [
    "## Deploy LLM on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c025e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name cu-classes-llm to get Role path.\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:iam::584072320137:user/cu-classes-llm is not authorized to perform: iam:GetRole on resource: role AmazonSageMaker-ExecutionRole-20240326T165029 because no identity-based policy allows the iam:GetRole action",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m \trole \u001b[38;5;241m=\u001b[39m \u001b[43msagemaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_execution_role\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sagemaker/session.py:7329\u001b[0m, in \u001b[0;36mget_execution_role\u001b[0;34m(sagemaker_session, use_default)\u001b[0m\n\u001b[1;32m   7325\u001b[0m message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   7326\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current AWS identity is not a role: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, therefore it cannot be used as a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   7327\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSageMaker execution role\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   7328\u001b[0m )\n\u001b[0;32m-> 7329\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message\u001b[38;5;241m.\u001b[39mformat(arn))\n",
      "\u001b[0;31mValueError\u001b[0m: The current AWS identity is not a role: arn:aws:iam::584072320137:user/cu-classes-llm, therefore it cannot be used as a SageMaker execution role",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m \tiam \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \trole \u001b[38;5;241m=\u001b[39m \u001b[43miam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_role\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRoleName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAmazonSageMaker-ExecutionRole-20240326T165029\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRole\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArn\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m \t\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name_or_path\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     13\u001b[0m \t\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/opt/ml/model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m \t\u001b[38;5;66;03m# add your remaining hyperparameters\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \t\u001b[38;5;66;03m# more info here https://github.com/huggingface/transformers/tree/v4.37.0/examples/pytorch/language-modeling\u001b[39;00m\n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# git configuration to download our fine-tuning script\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/botocore/client.py:1021\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:iam::584072320137:user/cu-classes-llm is not authorized to perform: iam:GetRole on resource: role AmazonSageMaker-ExecutionRole-20240326T165029 because no identity-based policy allows the iam:GetRole action"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-20240326T165029')['Role']['Arn']\n",
    "\t\t\n",
    "hyperparameters = {\n",
    "\t'model_name_or_path':'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "\t'output_dir':'/opt/ml/model'\n",
    "\t# add your remaining hyperparameters\n",
    "\t# more info here https://github.com/huggingface/transformers/tree/v4.37.0/examples/pytorch/language-modeling\n",
    "}\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.37.0'}\n",
    "\n",
    "# creates Hugging Face estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "\tentry_point='run_clm.py',\n",
    "\tsource_dir='./examples/pytorch/language-modeling',\n",
    "\tinstance_type='ml.p3.2xlarge',\n",
    "\tinstance_count=1,\n",
    "\trole=role,\n",
    "\tgit_config=git_config,\n",
    "\ttransformers_version='4.37.0',\n",
    "\tpytorch_version='2.1.0',\n",
    "\tpy_version='py310',\n",
    "\thyperparameters = hyperparameters\n",
    ")\n",
    "\n",
    "# starting the train job\n",
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbbb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3547b7",
   "metadata": {},
   "source": [
    "## Configure LLM in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\"max_new_tokens\": 512, \"top_p\": 0.8, \"temperature\": 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29135c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps(\n",
    "            # Mistral prompt, see https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
    "            {\"inputs\": f\"<s>[INST] {prompt} [/INST]\", \"parameters\": {**model_kwargs}}\n",
    "        )\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        splits = response_json[0][\"generated_text\"].split(\"[/INST] \")\n",
    "        return splits[1]\n",
    "\n",
    "content_handler = ContentHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker-runtime\") # needed for AWS credentials\n",
    "\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    content_handler=content_handler,\n",
    "    client=sm_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a7cb9",
   "metadata": {},
   "source": [
    "## Zero-shot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eec468",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "As a helpful energy specialist, please answer the question, focusing on numerical data.\n",
    "Don't invent facts. If you can't provide a factual answer, say you don't know what the answer is.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(system_prompt + \"{content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e5f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e58740",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the latest trend for solar investments in China?\"\n",
    "\n",
    "query = f\"question: {question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba91871",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm_chain.run({query})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45079dc5",
   "metadata": {},
   "source": [
    "## RAG example with PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b64b5bc",
   "metadata": {},
   "source": [
    "### Upload local PDF files to S3\n",
    "\n",
    "Sources:\n",
    "* https://www.iea.org/reports/world-energy-investment-2023\n",
    "* https://www.iea.org/reports/coal-2022\n",
    "* https://www.iea.org/reports/world-energy-outlook-2023\n",
    "\n",
    "Feel free to use your own files, the code below should work without any change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ba6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 bucket and prefix for PDF storage\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"langchain-rag-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17460881",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s $bucket $prefix\n",
    "aws s3 cp --recursive pdfs s3://$1/$2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b5ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of S3 URIs\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "objs = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "objs = objs['Contents']\n",
    "uris = [f's3://{bucket}/{obj[\"Key\"]}' for obj in objs]\n",
    "uris    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07745175",
   "metadata": {},
   "source": [
    "### Analyze documents with Amazon Textract and split them in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "textract_client = boto3.client('textract')\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=0)\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "for uri in uris:\n",
    "    loader = AmazonTextractPDFLoader(uri, client=textract_client)\n",
    "    document = loader.load()\n",
    "    chunks = splitter.split_documents(document)\n",
    "    all_chunks += chunks\n",
    "    print(f\"Loaded {uri}, {len(document)} pages, {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c74a8",
   "metadata": {},
   "source": [
    "### Embed document chunks and store them in FAISS\n",
    "https://github.com/facebookresearch/faiss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding model\n",
    "# See https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "embedding_model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Embed chunks\n",
    "embeddings_db = FAISS.from_documents(all_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3cbb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save database\n",
    "embeddings_db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb455a",
   "metadata": {},
   "source": [
    "### Shortcut : load existing embedding database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_db = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa22850",
   "metadata": {},
   "source": [
    "********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d4d47",
   "metadata": {},
   "source": [
    "### Configure RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = embeddings_db.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1db22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "prompt_template = \"\"\"\n",
    "As a helpful energy specialist, please answer the question below, focusing on numerical data and using only the context below.\n",
    "Don't invent facts. If you can't provide a factual answer, say you don't know what the answer is.\n",
    "\n",
    "question: {question}\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92ef004",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever, \n",
    "    chain_type_kwargs = {\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06143a",
   "metadata": {},
   "source": [
    "### Ask our question again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64223366",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the latest trend for solar investments in China?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a47bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What does STEPS mean?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d4337",
   "metadata": {},
   "source": [
    "## Delete endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066351f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f083366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
